{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be9a775f-dca6-4698-8f43-beee4b3a0a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Final, Dict, List, Union\n",
    "from ast import literal_eval\n",
    "from json import load\n",
    "from pyspark.sql import DataFrame, Column, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    to_date,\n",
    "    to_timestamp,\n",
    "    expr,\n",
    "    row_number,\n",
    "    max,\n",
    "    current_timestamp,\n",
    "    desc,\n",
    ")\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6a27c8-5dcd-44de-beea-f349586cc39a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"path_to_json\", '/Workspace/Repos/skyler.myers@sobeys.com/sobeys_dih_meta/integration-tests/sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "119e84cf-72af-45fd-91d8-ba756dde40e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CONFIGS: Final[Dict[str, Union[str, List[Dict[str, Union[str, List[Dict[str, str]]]]]]]] = (\n",
    "    load(open(dbutils.widgets.get(\"path_to_json\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c99764-17a4-486b-b453-adb4e942bcda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CONFIGS['catalog']};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80a1e18-6860-4ea2-84f5-1c461c2eac99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE SCHEMA {CONFIGS['source_schema']};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c889da-c6c6-4188-8a52-1a6c6e9fa7a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAPPING: Final[str] = CONFIGS[\"target_tables_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72cb09f-daad-4776-9827-5e7ab735d736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SOURCE_DF: Final[DataFrame] = spark.readStream.format(\"delta\").table(CONFIGS[\"source_table\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5616907-2f6a-4d0b-a2b2-016d1880576e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sinks(source_df: DataFrame, BATCH_ID: Final[int]) -> None:\n",
    "    for config in MAPPING:\n",
    "        clone_df: DataFrame = source_df\n",
    "        for operations in config.get(\"transformations\", []):\n",
    "            for column, transformation in operations.items():\n",
    "                clone_df: DataFrame = clone_df.withColumn(\n",
    "                    column, expr(transformation),\n",
    "                )\n",
    "\n",
    "        TS: Final[str] = config.get(\"timestamp_column\")\n",
    "        TARGET_TABLE: Final[str] = \".\".join([config.get(\"schema\"), config.get(\"table\")])\n",
    "        TARGET_DF: Final[DataFrame] = spark.table(TARGET_TABLE)\n",
    "        TARGET_COLS: Final[List[str]] = TARGET_DF.columns\n",
    "\n",
    "        latest: str = TARGET_DF.agg(max(col(TS))).first()[f\"max({TS})\"]\n",
    "        LATEST: Final[str] = latest if latest else \"0000-01-01T00:00:00.000Z\"\n",
    "        clone_df: DataFrame = clone_df.filter(col(CONFIGS[\"source_timestamp\"]) > LATEST)\n",
    "\n",
    "        SOURCE_PKS: Final[List[str]] = [\n",
    "            key.strip() for key in config.get(\"source_pks\").split(\",\")\n",
    "        ]\n",
    "        \n",
    "        clone_df: DataFrame = (\n",
    "            clone_df.withColumn(\n",
    "                \"rn\",\n",
    "                row_number().over(\n",
    "                    Window.partitionBy(*SOURCE_PKS).orderBy(\n",
    "                        desc(TS)\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            .filter(col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "        )\n",
    "\n",
    "        MODE: Final[str] = config.get(\"mode\")\n",
    "        if MODE == \"update\":\n",
    "            TARGET_PKS: Final[List[str]] = [\n",
    "                key.strip() for key in config.get(\"target_pks\").split(\",\")\n",
    "            ]\n",
    "            condition: List[str] = []\n",
    "            for source, target in zip(SOURCE_PKS, TARGET_PKS):\n",
    "                condition.append(f\"b.{source} = s.{target}\")\n",
    "\n",
    "            DeltaTable.forName(spark, TARGET_TABLE).alias(\"s\").merge(\n",
    "                clone_df.alias(\"b\"),\n",
    "                condition=\" and \".join(condition),\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "            clone_df.select(*TARGET_COLS).write.format(\"delta\").saveAsTable(\n",
    "                name=TARGET_TABLE,\n",
    "                mode=MODE,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44c334bf-a9ac-45e7-8ae2-ddd6fca11d87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream: DataFrame = SOURCE_DF.writeStream.format(\"delta\").option(\n",
    "    \"checkpointLocation\", CONFIGS[\"checkpoint_location\"],\n",
    ").trigger(availableNow=True).foreachBatch(write_to_sinks).queryName(\"ssot\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5563ab84-a7da-49b5-8df1-10b0c9e29089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5414014-241c-4b07-864b-287cae4bfa5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while stream.isActive:\n",
    "    print(stream.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac53f7b0-a0a5-4bdd-bea6-473d03d84ecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream.status"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ssot",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
