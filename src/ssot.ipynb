{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9a775f-dca6-4698-8f43-beee4b3a0a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Final, Dict, List, Union\n",
    "from ast import literal_eval\n",
    "from json import load\n",
    "from pyspark.sql import DataFrame, Column, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    to_date,\n",
    "    to_timestamp,\n",
    "    expr,\n",
    "    row_number,\n",
    "    max,\n",
    "    current_timestamp,\n",
    "    desc,\n",
    ")\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6a27c8-5dcd-44de-beea-f349586cc39a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"path_to_json\", \"/Volumes/dihpoc/ssot/mara/sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b8ab7d-e097-4f2c-b6b2-5b839747a8b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CONFIGS: Final[Dict[str, Union[str, List[Dict[str, Union[str, List[Dict[str, str]]]]]]]] = (\n",
    "    load(open(dbutils.widgets.get(\"path_to_json\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9edf53-d8ec-4454-bec5-855198b77cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CONFIGS['catalog']};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27bea5ad-4380-4050-922d-1d4bd6909bd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE SCHEMA {CONFIGS['source_schema']};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11a04bd-b1cc-4ddb-ba99-807ce14476fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAPPING: Final[List[Dict[str, Union[str, List[Dict[str, str]]]]]] = CONFIGS[\"target_tables_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8edb197e-4076-4d2e-859d-2582b46b7d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SOURCE_DF: Final[DataFrame] = spark.readStream.format(\"delta\").table(CONFIGS[\"source_table\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91726971-69b8-4993-a09e-a8bf84947ab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#     dbutils.fs.rm(\"/Volumes/dihpoc/ssot/mara/checkpoint\", True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c94a16e-bab3-4053-a3b9-f99c83988756",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/dihpoc/ssot/mara/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5616907-2f6a-4d0b-a2b2-016d1880576e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sinks(source_df: DataFrame, BATCH_ID: Final[int]) -> None:\n",
    "    print(\"111111111111111111111111111111\")\n",
    "    for config in MAPPING:\n",
    "        clone_df: DataFrame = source_df\n",
    "        print(\"222222222222222222222222222222222222222\")\n",
    "        for operations in config.get(\"transformations\", []):\n",
    "            for column, transformation in operations.items():\n",
    "                clone_df: DataFrame = clone_df.withColumn(\n",
    "                    column, expr(transformation),\n",
    "                )\n",
    "                \n",
    "        TS: Final[str] = config.get(\"timestamp_column\")\n",
    "        clone_df: DataFrame = clone_df.withColumnRenamed(CONFIGS[\"source_timestamp\"], TS)\n",
    "        TARGET_TABLE: Final[str] = \".\".join([config.get(\"schema\"), config.get(\"table\")])\n",
    "        TARGET_DF: Final[DataFrame] = source_df.sparkSession.table(TARGET_TABLE)\n",
    "        TARGET_COLS: Final[List[str]] = TARGET_DF.columns\n",
    "\n",
    "        latest: str = TARGET_DF.agg(max(col(TS))).first()[f\"max({TS})\"]\n",
    "        LATEST: Final[str] = latest if latest else \"0000-01-01T00:00:00.000Z\"\n",
    "        clone_df: DataFrame = clone_df.filter(col(CONFIGS[\"source_timestamp\"]) > LATEST)\n",
    "\n",
    "        SOURCE_PKS: Final[List[str]] = [\n",
    "            key.strip() for key in config.get(\"source_pks\").split(\",\")\n",
    "        ]\n",
    "        \n",
    "        clone_df: DataFrame = (\n",
    "            clone_df.withColumn(\n",
    "                \"rn\",\n",
    "                row_number().over(\n",
    "                    Window.partitionBy(*SOURCE_PKS).orderBy(\n",
    "                        desc(TS)\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            .filter(col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "        )\n",
    "        \n",
    "        print(\"44444444444444444444444444444444\")\n",
    "        MODE: Final[str] = config.get(\"mode\")\n",
    "        if MODE == \"update\":\n",
    "            TARGET_PKS: Final[List[str]] = [\n",
    "                key.strip() for key in config.get(\"target_pks\").split(\",\")\n",
    "            ]\n",
    "            condition: List[str] = []\n",
    "            for source, target in zip(SOURCE_PKS, TARGET_PKS):\n",
    "                condition.append(f\"b.{source} = s.{target}\")\n",
    "\n",
    "            DeltaTable.forName(source_df.sparkSession, TARGET_TABLE).alias(\"s\").merge(\n",
    "                clone_df.alias(\"b\"),\n",
    "                condition=\" and \".join(condition),\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "            clone_df.select(*TARGET_COLS).write.format(\"delta\").saveAsTable(\n",
    "                name=TARGET_TABLE,\n",
    "                mode=MODE,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c334bf-a9ac-45e7-8ae2-ddd6fca11d87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream: DataFrame = SOURCE_DF.writeStream.format(\"delta\").option(\n",
    "    \"checkpointLocation\", CONFIGS[\"checkpoint_location\"],\n",
    ").trigger(availableNow=True).foreachBatch(write_to_sinks).queryName(\"ssot\").start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ssot",
   "widgets": {
    "path_to_json": {
     "currentValue": "/Workspace/Repos/skyler.myers@sobeys.com/sobeys_dih_meta/integration-tests/sample.json",
     "nuid": "af7b4e3e-98bb-4877-9af3-68ac55a8bec2",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/dihpoc/ssot/mara/sample.json",
      "label": null,
      "name": "path_to_json",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
